# -*- coding: utf-8 -*-
"""OfflineRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nw_jNKrfr8xRV6IN7GamNM8SgP18jGfk

# Offline RAG Notebook
- In this notebook we have all the code that prepocceses raw text files, and retrives them based on given queries
"""

!pip install sentence-transformers
!pip install langchain
!pip install -U langchain-community
!pip install chromadb
!pip install langchain_cohere
!pip install tiktoken
!pip install nltk
!pip install langchain_groq
!pip install transformers
!pip install openai

import os
from dotenv import load_dotenv

# Load environment variables from '.env' file
load_dotenv()

# my personal api key, dont share with the world
os.environ['GROQ_API_KEY'] = "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"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

from google.colab import drive
drive.mount('/content/drive')

import json
import os

# retrival
from sentence_transformers import SentenceTransformer
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document


# splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt')


# relevency
import re
import openai
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_groq import ChatGroq
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import BertTokenizer, BertForSequenceClassification
import torch
from concurrent.futures import ThreadPoolExecutor, as_completed

from huggingface_hub import notebook_login

notebook_login()

"""# meta data extraction"""

# Function to extract metadata from the filename
def extract_metadata(file_name):
    print(file_name)
    # Assuming the structure is "category-subcategory-file_name.txt"
    if "-" in file_name:
        parts = file_name.rsplit("-", 2)  # Split the filename into category, subcategory, and file name
        category = parts[0].replace("_", " ")  # Replace underscores with spaces for category
        subcategory = parts[1].replace("_", " ")  # Replace underscores with spaces for subcategory
        title = parts[2].replace("_", " ").replace(".txt", "")  # Replace underscores with spaces and remove .txt
    else:
        category = "Unknown"
        subcategory = "Unknown"
        title = file_name.replace(".txt", "")  # Use the entire file name as the title if structure is different

    return title, category, subcategory

"""# Testing different spliting algorithms:
- simple split based on paragrpahs.
- more advanced using RecursiveCharacterTextSplitter
- HuggingFace - BERT split function
- HuggingFace - BERT split function + natural languge techniques
"""

def simple_paragraph_split(content):
    return [page for page in content.split('\n\n') if page.strip()]

def recursive_character_split(content, chunk_size=250, chunk_overlap=50):
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    return text_splitter.split_text(content)

def huggingface_token_split(content, max_token_size=256, overlap=50):
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer.tokenize(content)
    chunks = []
    for i in range(0, len(tokens), max_token_size - overlap):
        chunk = tokens[i:i + max_token_size]
        chunks.append(tokenizer.convert_tokens_to_string(chunk))
    return chunks

def huggingface_nltk_sentence_split(content, max_token_size=500, overlap=80):
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    sentences = sent_tokenize(content)
    current_chunk = []
    current_length = 0
    chunks = []
    for sentence in sentences:
        token_count = len(tokenizer.tokenize(sentence))
        if current_length + token_count > max_token_size:
            chunks.append(tokenizer.convert_tokens_to_string(current_chunk))
            current_chunk = current_chunk[-overlap:]
            current_length = len(tokenizer.tokenize(' '.join(current_chunk)))
        current_chunk.append(sentence)
        current_length += token_count
    if current_chunk:
        chunks.append(tokenizer.convert_tokens_to_string(current_chunk))
    return chunks

"""algorithm to manuly remove irrelevant text (titles, TOC, etc')"""

# def is_potentially_irrelevant(text):
#     """
#     Determines if a text chunk is likely irrelevant, such as titles, TOC entries, headers, or page numbers.

#     Parameters:
#         text (str): The text chunk to evaluate.

#     Returns:
#         bool: True if the chunk is likely irrelevant, False otherwise.
#     """

#     # Check for a high number of dots or dashes (indicates lines, separators)
#     if len(re.findall(r'[\.\-]{5,}', text)) > 0:
#         return True

#     # Check if the line ends with a number, likely a page number or section number
#     # Allows for optional whitespace or punctuation before the number
#     if re.search(r'[\s\.\-]*\d+\s*$', text):
#         return True

#     # Check for common title or TOC keywords
#     title_keywords = ["chapter", "contents", "table of contents", "section", "index"]
#     if any(keyword.lower() in text.lower() for keyword in title_keywords):
#         return True

#     # Exclude very short text with numbering like "1.", "1.1", etc.
#     # Lower the word count threshold to 10 to reduce false positives for brief relevant text
#     if len(text.split()) <= 10 or re.match(r"^\d+(\.\d+)*\s", text):
#         return True

#     # Check if most words are capitalized (indicating a title or TOC entry)
#     words = text.split()
#     capitalized_words = sum(1 for word in words if word.isupper())
#     if len(words) > 0 and capitalized_words >= len(words) / 2:
#         return True

#     # If none of the patterns match, the text is likely relevant
#     return False

# Precompiled regex patterns
NAVIGATION_PATTERN = re.compile(r'<.*?>')
VERSION_METADATA_PATTERN = re.compile(r'SEBOK v\.\s*\d+\.\d+\.\d+|released \d{1,2} \w+ \d{4}', re.IGNORECASE)
FIGURE_PATTERN = re.compile(r'Figure\s+\d+.*|^Page\s+\d+$', re.IGNORECASE)
REFERENCES_PATTERN = re.compile(r'^References\s*\nNone\.$', re.IGNORECASE)
DOTS_DASHES_PATTERN = re.compile(r'[\.\-]{5,}')
ENDS_WITH_NUMBER_PATTERN = re.compile(r'[\s\.\-]\d+\s$')
NUMBERING_PATTERN = re.compile(r"^\d+(\.\d+)*\s")
BIBLIOGRAPHIC_PATTERN = re.compile(
    r'(ISBN|pp\.|Ver\.|Technical Report|Handbook|Edition|CRC Press|Taylor & Francis|MITRE|INCOSE)', re.IGNORECASE
)
STOP_WORDS = {"the", "of", "and", "to", "in", "on", "with", "a", "is"}

def is_potentially_irrelevant(text, title_keywords=None):
    if title_keywords is None:
        title_keywords = ["chapter", "contents", "table of contents", "section", "index"]

    # Check for navigation hints and version metadata
    if NAVIGATION_PATTERN.search(text) or VERSION_METADATA_PATTERN.search(text):
        return True

    # Check for figure labels, page numbers, and empty references
    if FIGURE_PATTERN.search(text) or REFERENCES_PATTERN.search(text):
        return True

    # Check for a high number of dots or dashes
    if DOTS_DASHES_PATTERN.search(text):
        return True

    if sum(c == '.' for c in text) / len(text) > 0.2 or sum(c == 'â€¢' for c in text) / len(text) > 0.2:
        return True

    # Check if the line ends with a number
    if ENDS_WITH_NUMBER_PATTERN.search(text):
        return True

    # Check for bibliographic patterns
    if BIBLIOGRAPHIC_PATTERN.search(text):
        return True

    # Check for common title or TOC keywords
    if any(keyword.lower() in text.lower() for keyword in title_keywords):
        return True

    # Exclude very short text with numbering
    if NUMBERING_PATTERN.match(text):
        return True

    # Check for excessive capitalization
    words = [word for word in text.split() if word.lower() not in STOP_WORDS]
    capitalized_words = sum(1 for word in words if word.isupper())
    if len(words) > 0 and capitalized_words >= len(words) / 2:
        return True

    # Exclude chunks that are too short or too long
    if len(text.split()) < 15 or len(text) > 1000:
        return True

    # If none match, consider it relevant
    return False

def process_documents_with_split(directory_path, split_method):
    documents = []

    for file_name in os.listdir(directory_path):
        if file_name.endswith(".txt"):
            file_path = os.path.join(directory_path, file_name)

            # Extract metadata
            title, category, subcategory = extract_metadata(file_name)

            # Read the content of the file
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

            # Use the specified split method
            split_content = split_method(content)

            # Filter out irrelevant chunks
            split_content = [chunk for chunk in split_content if not is_potentially_irrelevant(chunk)]

            # Wrap each split chunk as a Document with metadata
            documents += [Document(page_content=chunk, metadata={"title": title, "category": category, "sub_category": subcategory})
                          for chunk in split_content]

    print(f"Processed {len(documents)} documents using {split_method.__name__}.")
    return documents

doc_dir_path = "/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resouces/RAG_Documents"
clues_dir_path = "/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resouces/Holding_Cell"

# simple_split = process_documents_with_split(clues_dir_path, simple_paragraph_split)
recursive_split = process_documents_with_split(doc_dir_path, recursive_character_split)
# huggingface_split = process_documents_with_split(doc_dir_path, huggingface_token_split)
# huggingface_nltk = process_documents_with_split(doc_dir_path, huggingface_nltk_sentence_split)

for doc in recursive_split[1:2400:100]:
  print("#########################################")
  print(doc.page_content)

"""save chunked text based on splitting method"""

# Specify the output file path
output_file_path = '/content/processed_chunks.txt'

# Open the file in write mode
with open(output_file_path, 'w', encoding='utf-8') as output_file:
    for doc in recursive_split:
        output_file.write("-----------------------\n")
        output_file.write(doc.page_content + "\n")
        output_file.write("-----------------------\n")

print(f"All chunks have been saved to {output_file_path}")

"""# Create Documents object from split text files

method that extract just the question from the query.
- ment to prevent retrival of data that's similar to wrong answer
"""

def extract_question(query_string):
    start_marker = "Question: "
    end_marker_options = ["\nA.", "\nAnswer:"]

    # Find the start of the question
    start_index = query_string.find(start_marker)
    if start_index == -1:
        start_index = 0
    else:
        start_index += len(start_marker)

    # Find the earliest occurrence of any end marker
    end_indices = [query_string.find(em, start_index) for em in end_marker_options]
    end_indices = [idx for idx in end_indices if idx != -1]
    if end_indices:
        end_index = min(end_indices)
    else:
        end_index = len(query_string)

    question_text = query_string[start_index:end_index].strip()
    return question_text

"""# LLM as a Judge"""

import torch

# Define LLM-based relevance model
class GradeDocuments(BaseModel):
    binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")

# Initialize LLM model for relevance grading
llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

# Define prompt template
system_prompt = """You are a grader assessing relevance of a retrieved document to a user question.
If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.
It does not need to be a stringent test. The goal is to filter out erroneous retrievals.
Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""
grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "Retrieved document: \n\n {document} \n\n User question: {question}")
])
retrieval_grader = grade_prompt | structured_llm_grader

# Helper function to check LLM-based relevance
def llm_relevance_check(doc, question):
    torch.cuda.empty_cache()
    response = retrieval_grader.invoke({"question": question, "document": doc.page_content})
    return response.binary_score == 'yes'

# system_prompt = """You are an evaluator determining if a document provides context that directly supports answering a user's question.
# Your goal is to select documents that contain information useful for answering the question accurately. This includes relevant explanations, supporting details, or related information that would aid in constructing a comprehensive answer.
# Relevance Criteria:
#  **Direct Answer Support**: The document should provide information that contributes to a clear understanding of the question or helps directly address it.
# Decision Instructions:
# - Answer 'yes' if the document contains context or details that would directly help in answering the question.
# - Answer 'no' if the document does not contribute useful information for answering the question.
# Provide only 'yes' or 'no' based on these criteria."""

system_prompt = """You are a grader assessing relevance of a retrieved document to a user question.
If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.
It does not need to be a stringent test. The goal is to filter out erroneous retrievals.
Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

"""Lamma_3_3B"""

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model
model_name = "meta-llama/Llama-3.2-3B-Instruct"  # Adjust based on the specific Llama 3 model you intend to use
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def check_relevance_llama3(document, query):
    document_content = document.page_content  # Ensure this is the correct attribute for your document's content
    prompt = f"{system_prompt}\nQuestion: {query}\nDocument: {document_content}"
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    output = model.generate(**inputs, max_new_tokens=5, do_sample=False)
    answer = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()
    is_relevant = "yes" in answer
    return is_relevant

"""Bert"""

def check_relevance_bert(document, query):
    document_content = document.page_content  # Ensure this is the correct attribute for your document's content
    prompt_text = f"{system_prompt}\nQuestion: {query}\nDocument: {document_content}"
    inputs = tokenizer.encode_plus(prompt_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model(**inputs)

    # Obtain scores and decide relevance
    scores = torch.softmax(outputs.logits, dim=1)
    is_relevant = scores[:, 1].item() > 0.6  # Assuming the second label corresponds to "Yes"
    if is_relevant:
      print(f"Query: {query}\nDocument snippet: {document_content[:5]}... Score for Relevance: {scores[:, 1].item()}\n\n")

    return is_relevant

"""GPT"""

import openai
key = "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"
from openai import OpenAI

client = OpenAI(
    api_key = key
)

def check_relevance_with_gradual_scores(document, query):
    prompt = f"""
    You are a systems engineering assistant. Assess if the retrieved document helps answer the query.
    Options: Relevant, Partially Relevant, Irrelevant.

    Query: "{query}"
    Document: "{document.page_content}"
    """
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are an expert assistant evaluating relevance."},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )
    relevance = response.choices[0].message.content.strip().lower()
    return relevance in ["relevant", "partially relevant"]

def check_relevance_with_chatgpt(document, query):
    relevance_prompt_template = """
    You are a systems engineering assistant. Your goal is to assess if the retrieved document provides the specific information required to answer the query accurately. Focus on whether the document includes details that directly support or relate to the correct answer choice.
    Query: "{query}"
    Retrieved Document: "{retrieved_text}"
    Based on the information in the retrieved document, can it help answer the query correctly? Answer 'true' if the document provides relevant and answer-specific details, or 'false' if it does not. Briefly explain your choice, focusing on whether the document directly addresses concepts related to the correct answer choice.
    """
    document_content = document.page_content  # Ensure this aligns with your document's attribute structure
    prompt = relevance_prompt_template.format(query=query, retrieved_text=document_content)

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # or "gpt-3.5-turbo" for faster, less expensive evaluations
        messages=[
            {"role": "system", "content": "You are an expert assistant evaluating relevance."},
            {"role": "user", "content": prompt}
        ],
        temperature=0  # Use 0 for deterministic output
    )

    # Extract the assistant's response
    relevance_answer = response.choices[0].message.content.strip()
    is_relevant = "yes" in relevance_answer.lower()

    return is_relevant

"""# Retrive k document per query for all queries in the benchmark
- saves retrived fils in the RAG-eval foramt in provided output dir
"""

print("Starting process_queries_and_save_retrievals")
embeddings = SentenceTransformerEmbeddings(model_name='all-mpnet-base-v2')
print("Embeddings initialized")
vector_store = Chroma.from_documents(recursive_split, embedding=embeddings)
print("Vector store created")

# def process_queries_and_save_retrievals(documents, name, query_file_path, output_dir="/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resources/retrieved_documents", model_name='all-mpnet-base-v2', with_relevancy = False):
#     with open(query_file_path, 'r') as f:
#         queries = [json.loads(line)["query"] for line in f]
#     retrieved_data = []
#     retrieved_cache = {}  # Cache to store retrieved documents for each unique question
#     i = 0
#     for query in queries:
#         i += 1
#         print(f"Processing query {i}/{len(queries)}\n")
#         print("  ",query)
#         # Extract only the question part for retrieval
#         question = extract_question(query)

#         # Check if we already retrieved documents for this question
#         if question in retrieved_cache:
#             relevant_docs = retrieved_cache[question]
#         else:
#             # Retrieve top-k documents for the query
#             retriever = vector_store.as_retriever(
#                 search_type="similarity",
#                 search_kwargs={'k': 4}  # number of documents to retrieve
#             )
#             retrieved_docs = retriever.invoke(query)

#             # # Filter out duplicate documents based on page_content
#             unique_docs = []
#             seen_texts = set()
#             for doc in retrieved_docs:
#                 if doc.page_content not in seen_texts:
#                     unique_docs.append(doc)
#                     seen_texts.add(doc.page_content)
#             # Apply relevance check

#             ############################################################
#             # gpt suggestion
#             # Define the framing context and CoT reasoning context
#             # cot_reasoning_context = (
#             #     "Based on the retrieved information, evaluate its relevance to the question. "
#             #     "If the information is relevant, analyze it step-by-step to answer the question. "
#             #     "If it is not relevant, disregard it."
#             # )
#             #
#             # retrieved_data.append({
#             #     "query": query,
#             #     "ctxs": [
#             #         {"retrieval text": (framing_context + + doc.page_content) if idx == 0 else (cot_reasoning_context + " " + doc.page_content)}
#             #         for idx, doc in enumerate(relevant_docs)
#             #     ]
#             # })
#             ############################################################

#             if with_relevancy:
#               # if we want to run the llm relevency check
#               relevant_docs = [doc for doc in unique_docs if check_relevance_with_gradual_scores(doc, query)]
#             else:
#               # if we dont want to run the llm relevency check
#               relevant_docs = unique_docs
#               for doc in unique_docs:
#                 print(doc.page_content)

#             # Store retrieved documents in the cache
#             retrieved_cache[question] = relevant_docs
#         # Format the retrieved documents into the required format for RAG evaluation
#         framing_context = "Imagine you are a Systems Engineering expert. "
#         cot_reasoning_context = (
#                 "Based on the retrieved information, evaluate the following carefully. "
#                 "Analyze each piece of information step-by-step to answer the question:"
#         )
#         retrieved_data.append({
#             "query": query,
#             # "ctxs": [{"retrieval text": doc.page_content} for doc in relevant_docs]
#             "ctxs": [
#               {"retrieval text": (framing_context + cot_reasoning_context + "\n" + doc.page_content + "\n") if idx == 0 else (cot_reasoning_context + "\n" + doc.page_content + "\n")}
#               for idx, doc in enumerate(relevant_docs)]
#         })
#         rev_page_cont = [doc.page_content for doc in relevant_docs]
#         print(f"Finished processing query {i}/{len(queries)}\n retrived docs {rev_page_cont}")

#     # Use the 'name' parameter for a concise filename
#     output_file_path = os.path.join(output_dir, f"{name}_retrieved_documents.jsonl")

#     # Save the retrieved data in JSONL format for RAG evaluation
#     with open(output_file_path, 'w') as f:
#         for item in retrieved_data:
#             f.write(json.dumps(item) + '\n')

#     print(f"Retrieved documents saved successfully to {output_file_path}!")

def process_queries_and_save_retrievals(
    documents,
    name,
    query_file_path,
    output_dir="/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resources/retrieved_documents",
    model_name='all-mpnet-base-v2',
    with_relevancy=False,
    verbose=True
):
    with open(query_file_path, 'r') as f:
        queries = [json.loads(line)["query"] for line in f]

    retrieved_data = []
    retrieved_cache = {}  # Cache to store retrieved documents for each unique question
    i = 0

    for query in queries:
        i += 1
        print(f"Processing query {i}/{len(queries)}\n{'='*50}")
        print(f"Query: {query}")

        # Extract only the question part for retrieval
        question = extract_question(query)

        # Check if we already retrieved documents for this question
        if question in retrieved_cache:
            relevant_docs = retrieved_cache[question]
        else:
            # Retrieve top-k documents for the query in this case k =4
            retriever = vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={'k': 4}  # Number of documents to retrieve
            )
            retrieved_docs = retriever.invoke(query)

            # Filter out duplicate documents based on page_content
            unique_docs = []
            seen_texts = set()
            for doc in retrieved_docs:
                if doc.page_content not in seen_texts:
                    unique_docs.append(doc)
                    seen_texts.add(doc.page_content)

            # Apply relevance check
            if with_relevancy:
                print("Applying relevance checks...")
                relevant_docs = [doc for doc in unique_docs if check_relevance_with_gradual_scores(doc, query)]
            else:
                relevant_docs = unique_docs

            # Print retrieved documents (optional verbose mode)
            if verbose:
                print(f"Retrieved Documents for Query {i} (with_relevancy={with_relevancy}):")
                for idx, doc in enumerate(relevant_docs):
                    print(f"  Document {idx+1}: {doc.page_content[:200]}...")  # Print a preview of the content

            # Store retrieved documents in the cache
            retrieved_cache[question] = relevant_docs

        # Format the retrieved documents into the required format for RAG evaluation
        framing_context = "Imagine you are a Systems Engineering expert. "
        cot_reasoning_context = (
            "Based on the retrieved information, evaluate the following carefully. "
            "Analyze each piece of information step-by-step to answer the question:"
        )
        cot_reasoning_context_empty_RAG = (
            "Think thoroughly about the question before answering."
        )

        # Check if relevant_docs is empty
        if not relevant_docs:
            # Handle case with no relevant documents
            retrieved_data.append({
                "query": query,
                "ctxs": [
                    {
                        "retrieval text": (
                            "Instruction: " + framing_context + cot_reasoning_context_empty_RAG + "\n"
                        )
                    }
                ]
            })
        else:
            # Handle case with relevant documents
            retrieved_data.append({
                "query": query,
                "ctxs": [
                    {
                        "retrieval text": (
                            "Instruction: " + framing_context + cot_reasoning_context + "\n" + "Context:\n" + doc.page_content + "\n"
                            if idx == 0
                            else cot_reasoning_context + "\n" + doc.page_content + "\n"
                        )
                    }
                    for idx, doc in enumerate(relevant_docs)
                ]
            })

        # Print summary of processed query
        if verbose:
            print(f"Finished processing Query {i}/{len(queries)}")
            print(f"Retrieved {len(relevant_docs)} relevant documents.")
            print("="*50)

    # Use the 'name' parameter for a concise filename
    output_file_path = os.path.join(output_dir, f"{name}_retrieved_documents.jsonl")

    # Save the retrieved data in JSONL format for RAG evaluation
    with open(output_file_path, 'w') as f:
        for item in retrieved_data:
            f.write(json.dumps(item) + '\n')

    print(f"Retrieved documents saved successfully to {output_file_path}!")

# input_path_1 = "/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resouces/benchmark_inputs/partial_sysengbench_start_at_1st.jsonl"
input_path_5 = "/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resouces/benchmark_inputs/partial_sysengbench_start_at_5th.jsonl"
# input_path_3 = "/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resouces/benchmark_inputs/partial_sysengbench_start_at_3rd.jsonl"

output_path = "/content/drive/MyDrive/Technion - LLM Research 2024/Coding Environment /Resources/RAG resouces/retrived_documents/partial_benchmark"
a = "/content"
# process_queries_and_save_retrievals(recursive_split, "start_at_1st_full_corpus_gpt_judge", input_path_1, output_path, with_relevancy=True)
process_queries_and_save_retrievals(recursive_split, "recursive_paragraph_split_full_corpus_start_at_5", input_path_5, output_path, with_relevancy=True)
# process_queries_and_save_retrievals(recursive_split, "recursive_paragraph_split_full_corpus_start_at_3", input_path_3, output_path)

import json
import os

def add_framing_to_empty_ctxs(file_path):
    # Define the framing and reasoning contexts
    framing_context = "Imagine you are a Systems Engineering expert. "
    cot_reasoning_context_empty_RAG = (
        "Think thoroughly about the question before answering"
    )

    updated_data = []

    # Read the existing JSONL file
    with open(file_path, 'r') as f:
        for line in f:
            entry = json.loads(line)

            # Check if the ctxs is empty
            if not entry.get("ctxs"):
                # Add a placeholder with framing and reasoning contexts
                entry["ctxs"] = [{
                    "retrieval text": framing_context + cot_reasoning_context_empty_RAG
                }]

            updated_data.append(entry)

    # Save the updated JSONL data back to the file
    with open(file_path, 'w') as f:
        for item in updated_data:
            f.write(json.dumps(item) + '\n')

    print(f"Updated entries with empty ctxs in {file_path}.")

# Example Usage
file_path = "/content/start_at_1st_full_corpus_gpt_judge_retrieved_documents.jsonl"
add_framing_to_empty_ctxs(file_path)

# Specify the path to the input and output files
input_file_path = '/content/corpus_used.txt'
output_file_path = '/content/cleaned_corpus.txt'

# Open the input file and read lines
with open(input_file_path, 'r') as file:
    lines = file.readlines()

# Filter out empty or whitespace-only lines
cleaned_lines = [line for line in lines if line.strip()]

# Open the output file and write the cleaned lines
with open(output_file_path, 'w') as file:
    file.writelines(cleaned_lines)